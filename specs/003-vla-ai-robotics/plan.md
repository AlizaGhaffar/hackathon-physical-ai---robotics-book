# Implementation Plan: Vision-Language-Action: AI Meets Robotics (Chapter 3)

**Branch**: `003-vla-ai-robotics` | **Date**: 2025-12-03 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/003-vla-ai-robotics/spec.md`

## Summary

Chapter 3 implements the climax of the Physical AI textbook - a complete Vision-Language-Action (VLA) pipeline that enables learners to control simulated robots through voice commands, leveraging GPT-4 Vision for scene understanding and LLM-based task planning. The chapter culminates in a capstone project ("Autonomous Humanoid Assistant") that integrates all three chapters' concepts. This is the primary demo showcase for hackathon judging, optimized for a 90-second presentation.

**Technical Approach**: Build FastAPI backend as bridge between web frontend (React/Docusaurus) and ROS 2 network. OpenAI APIs (Whisper, GPT-4, GPT-4 Vision) handle AI processing with safety validation layer enforcing constraints before robot execution. Multi-modal RAG chatbot (Qdrant) provides learning assistance.

## Technical Context

**Language/Version**: Python 3.11+ (backend), TypeScript 5.0+ (frontend), ROS 2 Humble (robot system)
**Primary Dependencies**: FastAPI 0.109+, OpenAI SDK 1.3+, rclpy (ROS 2 Python client), Docusaurus 3.0+, Monaco Editor 0.45+, Framer Motion 11.0+
**Storage**: Neon Postgres (user data, capstone progress), Qdrant Cloud (vector embeddings for RAG), Redis (optional, for caching Whisper/Vision results)
**Testing**: pytest (backend unit/integration), Jest (frontend components), ROS 2 integration tests (Gazebo simulation)
**Target Platform**: Web browsers (Chrome/Edge/Firefox latest 2 versions), Linux server (Ubuntu 22.04) for backend, ROS 2 Humble environment for simulation
**Project Type**: Web application (frontend + backend + ROS 2 bridge)
**Performance Goals**:
- Voice-to-action latency <3 seconds (p95)
- Vision scene analysis <5 seconds
- Code generation <8 seconds with streaming
- Gazebo simulation ‚â•20 FPS
- RAG chatbot response <3 seconds (text), <5 seconds (multi-modal)
**Constraints**:
- OpenAI API rate limits (TPM quotas)
- Free tier limits: Qdrant (1GB, 100K vectors), Neon (0.5GB storage, 100 hours compute/month)
- Gazebo stability (prone to crashes, mitigation strategies required)
- Browser MediaRecorder API compatibility (WebM format, not all browsers)
- 90-second demo time limit (every feature must be demoable quickly)
**Scale/Scope**:
- ~500-1000 concurrent learners (hackathon scale)
- Chapter 3 content: 50 sections + 20 code examples + 15 diagrams = 85 vectors
- Capstone project: 6 modular phases, 2-3 hour completion target
- 6 prioritized user stories (3xP1, 2xP2, 1xP3)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Principles Compliance

**I. Multi-Chapter Architecture** ‚úÖ
- Reuses Chapters 1-2 architecture (Docusaurus, FastAPI, Neon, better-auth)
- Chapter 3 independently functional (can start from Ch3 if user has ROS 2 + Gazebo knowledge)
- Navigation integrated (Chapter 3 card added to homepage)

**VIII. Climax Chapter Excellence** ‚úÖ
- Most polished chapter (production-grade code, no TODOs in user-facing content)
- Primary demo chapter (90-second script optimized)
- Exceeds Ch1-2 in interactivity (voice UI, vision overlay, live code execution)

**IX. AI-Robotics Convergence** ‚úÖ
- LLM-controlled robots (Whisper ‚Üí GPT-4 ‚Üí ROS 2 actions)
- GPT-4 Vision integration (scene understanding, spatial reasoning)
- Code generation (natural language ‚Üí ROS 2 Python)
- Multi-modal RAG chatbot

**X. Capstone Project Readiness** ‚úÖ
- Integrates Ch1 (ROS 2 nodes), Ch2 (Gazebo simulation), Ch3 (VLA pipeline)
- 6 modular phases (each independently testable)
- 2-3 hour completion target
- Step-by-step guide with troubleshooting

**XI. Demo Showcase Optimization** ‚úÖ
- 90-second demo script (voice ‚Üí vision ‚Üí action, smooth transitions)
- Pre-recorded fallback video (risk mitigation)
- All bonus features highlighted (personalization, Urdu translation, auth, RAG chatbot)

**XII. Future-Proof Architecture** ‚úÖ
- Multi-modal RAG (text + images + code embeddings)
- Abstracted AI service layer (easy to swap OpenAI ‚Üí Claude/other LLMs)
- Hybrid search (dense + sparse, Qdrant)
- Graceful degradation (3-tier error handling)

**Technical Standards Compliance**:
- ‚úÖ OpenAI APIs used (Whisper, GPT-4, GPT-4 Vision) per hackathon requirements
- ‚úÖ FastAPI backend + Docusaurus frontend (existing stack)
- ‚úÖ Neon Postgres + Qdrant Cloud (existing databases)
- ‚úÖ better-auth.com authentication (bonus feature continuity)
- ‚úÖ GitHub Pages / Vercel deployment ready

**Quality Gates (Pre-Deployment)**:
- [ ] All P1 user stories functional (voice control, vision manipulation, capstone)
- [ ] RAG chatbot 85%+ accuracy on 50 Chapter 3 test questions
- [ ] Capstone project completable in 2-3 hours (validated with 3 test learners)
- [ ] 90-second demo rehearsed (no delays, smooth transitions)
- [ ] All 5 code examples executable (copy-paste-run works)
- [ ] Safety validator blocks 100% of dangerous commands (test set: 20 unsafe commands)
- [ ] Visual consistency validated (same UI components as Ch1-2)
- [ ] Mobile responsive (tablets 768px+)
- [ ] No console errors in DevTools

**Post-Design Re-Check** (After Phase 1):
- ‚úÖ No unnecessary complexity added (all tech choices justified in research.md)
- ‚úÖ Component reusability maintained (PersonalizeButton, TranslateButton, ChatBot reused)
- ‚úÖ Database schema uses chapter identifiers (not chapter-specific tables)

---

## Project Structure

### Documentation (this feature)

```text
specs/003-vla-ai-robotics/
‚îú‚îÄ‚îÄ plan.md              # This file
‚îú‚îÄ‚îÄ spec.md              # Feature specification (43 requirements, 20 success criteria)
‚îú‚îÄ‚îÄ research.md          # Technology decisions (10 key decisions + best practices)
‚îú‚îÄ‚îÄ data-model.md        # Database schema (8 core entities + relationships)
‚îú‚îÄ‚îÄ quickstart.md        # Developer quickstart guide (13-day timeline)
‚îú‚îÄ‚îÄ contracts/
‚îÇ   ‚îî‚îÄ‚îÄ api-specification.yaml  # OpenAPI 3.0 (30+ endpoints)
‚îú‚îÄ‚îÄ checklists/
‚îÇ   ‚îî‚îÄ‚îÄ requirements.md  # Specification quality validation (14/14 passed)
‚îî‚îÄ‚îÄ tasks.md             # TO BE GENERATED by /sp.tasks command
```

### Source Code (repository root)

```text
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ voice.py          # /voice/* endpoints (Whisper integration)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vision.py         # /vision/* endpoints (GPT-4 Vision)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ robot.py          # /robot/* endpoints (command processing)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code.py           # /code/* endpoints (AI code generation)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ capstone.py       # /capstone/* endpoints (project management)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ playground.py    # /playground/* endpoints (experimentation)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ auth.py           # JWT validation (better-auth integration)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ rate_limit.py    # OpenAI API quota management
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ whisper_client.py    # OpenAI Whisper API wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt4_client.py       # GPT-4 API wrapper (code gen, command planning)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt4_vision_client.py # GPT-4 Vision API wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ safety_validator.py  # Robot action safety checks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ros2_bridge.py       # rclpy integration (publish to ROS 2 topics)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code_executor.py     # Docker sandbox for user code
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qdrant_client.py     # Vector store for RAG (Chapter 3 embeddings)
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ voice_command.py     # SQLAlchemy model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visual_scene.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_query.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ robot_action.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ task_plan.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generated_code.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ capstone_project.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ playground_experiment.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (Pydantic schemas matching OpenAPI spec)
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ cache.py             # Redis caching for Whisper/Vision results
‚îÇ       ‚îî‚îÄ‚îÄ error_handlers.py    # 3-tier graceful degradation
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ unit/
    ‚îÇ   ‚îú‚îÄ‚îÄ test_whisper_client.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_safety_validator.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_code_executor.py
    ‚îú‚îÄ‚îÄ integration/
    ‚îÇ   ‚îú‚îÄ‚îÄ test_voice_to_robot_pipeline.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_vision_pipeline.py
    ‚îî‚îÄ‚îÄ e2e/
        ‚îî‚îÄ‚îÄ test_capstone_walkthrough.py

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chapter3/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VoiceCommandUI.tsx        # Microphone button, waveform viz
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VisionOverlay.tsx         # Object labels on Gazebo screenshot
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CodeEditor.tsx            # Monaco Editor wrapper
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RobotControlPanel.tsx     # Manual control fallback
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CapstoneProgress.tsx      # 6-phase progress tracker
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PlaygroundUI.tsx          # Prompt engineering sandbox
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SafetyIndicator.tsx       # Real-time safety status
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shared/ (from Ch1-2)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ PersonalizeButton.tsx     # Reused
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ TranslateButton.tsx       # Reused
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ RAGChatbot.tsx            # Enhanced for multi-modal
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ProgressTracker.tsx       # Chapter 3 integration
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useVoiceRecorder.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useRobotState.ts             # WebSocket connection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useCodeExecution.ts
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.ts                       # Axios wrapper (typed from OpenAPI)
‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ       ‚îî‚îÄ‚îÄ chapter3/ (Docusaurus markdown + React components)
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ components/
        ‚îú‚îÄ‚îÄ VoiceCommandUI.test.tsx
        ‚îî‚îÄ‚îÄ CodeEditor.test.tsx

docs/
‚îî‚îÄ‚îÄ chapter-3/
    ‚îú‚îÄ‚îÄ intro.md
    ‚îú‚îÄ‚îÄ what-is-vla.md
    ‚îú‚îÄ‚îÄ voice-control/
    ‚îÇ   ‚îú‚îÄ‚îÄ whisper-integration.md
    ‚îÇ   ‚îî‚îÄ‚îÄ natural-language-commands.md
    ‚îú‚îÄ‚îÄ vision/
    ‚îÇ   ‚îú‚îÄ‚îÄ gpt4-vision.md
    ‚îÇ   ‚îú‚îÄ‚îÄ scene-understanding.md
    ‚îÇ   ‚îî‚îÄ‚îÄ spatial-reasoning.md
    ‚îú‚îÄ‚îÄ task-planning/
    ‚îÇ   ‚îú‚îÄ‚îÄ llm-based-planning.md
    ‚îÇ   ‚îú‚îÄ‚îÄ safety-constraints.md
    ‚îÇ   ‚îî‚îÄ‚îÄ error-recovery.md
    ‚îú‚îÄ‚îÄ examples/
    ‚îÇ   ‚îú‚îÄ‚îÄ example-01-voice-navigation.md
    ‚îÇ   ‚îú‚îÄ‚îÄ example-02-vision-manipulation.md
    ‚îÇ   ‚îú‚îÄ‚îÄ example-03-code-generation.md
    ‚îÇ   ‚îú‚îÄ‚îÄ example-04-task-planning.md
    ‚îÇ   ‚îî‚îÄ‚îÄ example-05-capstone-demo.md
    ‚îú‚îÄ‚îÄ exercises/
    ‚îÇ   ‚îú‚îÄ‚îÄ exercise-01-voice-commands.md
    ‚îÇ   ‚îú‚îÄ‚îÄ exercise-02-vision-queries.md
    ‚îÇ   ‚îú‚îÄ‚îÄ exercise-03-prompt-engineering.md
    ‚îÇ   ‚îî‚îÄ‚îÄ capstone-project.md
    ‚îî‚îÄ‚îÄ troubleshooting.md

ros2_workspace/ (optional, for capstone)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ vla_control/
‚îÇ       ‚îú‚îÄ‚îÄ launch/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ capstone.launch.py
‚îÇ       ‚îú‚îÄ‚îÄ config/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ safety_params.yaml
‚îÇ       ‚îî‚îÄ‚îÄ scripts/
‚îÇ           ‚îú‚îÄ‚îÄ voice_listener.py
‚îÇ           ‚îú‚îÄ‚îÄ vision_processor.py
‚îÇ           ‚îî‚îÄ‚îÄ action_executor.py
‚îî‚îÄ‚îÄ README.md

docker/
‚îú‚îÄ‚îÄ code_sandbox.Dockerfile     # For user code execution (ROS 2 Humble base)
‚îî‚îÄ‚îÄ docker-compose.yml          # Full stack (backend + Gazebo + Postgres + Redis)
```

**Structure Decision**: Web application structure chosen (frontend + backend). ROS 2 workspace is optional (provided for capstone project but not required for textbook operation). Docker used for code sandbox isolation.

---

## Complexity Tracking

> No constitution violations requiring justification. All complexity is necessitated by feature requirements (AI integration, ROS 2 bridge, safety validation, capstone project).

---

## Implementation Phases

### Phase 0: Research & Technology Decisions ‚úÖ COMPLETE

**Deliverables**:
- [x] `research.md` with 10 key decisions (Whisper API, GPT-4, GPT-4 Vision, ROS 2 bridge architecture, Monaco Editor, capstone structure, demo strategy, Qdrant hybrid search, Framer Motion, error handling)
- [x] Best practices researched (LLM safety prompts, Gazebo performance, OpenAI API optimization)
- [x] Risks identified & mitigated (rate limits, Gazebo crashes, voice recognition failures, LLM hallucinations)

**Key Decisions Made**:
1. OpenAI Whisper API (cloud, not local) for <2s latency
2. GPT-4 (not GPT-3.5) for better reasoning & safety
3. GPT-4 Vision API (not custom CV) for zero-shot scene understanding
4. FastAPI bridge (not direct browser-ROS connection) for security & API key management
5. Monaco Editor (not CodeMirror) for VS Code familiarity
6. 6 modular capstone phases (not all-or-nothing) for incremental success
7. Pre-recorded demo fallback (not live-only) for risk mitigation
8. Qdrant hybrid search (dense + sparse) for better RAG retrieval
9. Framer Motion (not CSS animations) for smooth 60fps UI
10. 3-tier error handling (retry ‚Üí cache ‚Üí manual fallback)

---

### Phase 1: Data Models & API Contracts ‚úÖ COMPLETE

**Deliverables**:
- [x] `data-model.md` with 8 core entities (VoiceCommand, VisualScene, LLMQuery, RobotAction, TaskPlan, GeneratedCode, CapstoneProject, PlaygroundExperiment)
- [x] ERD relationship diagram (one-to-many, many-to-one mappings)
- [x] Field constraints & validation rules
- [x] JSONB structures for flexible data (action parameters, object lists, experiment settings)
- [x] State machines (status transitions for async operations)
- [x] Database indexes (performance-critical queries)
- [x] Data retention policies (privacy & storage optimization)
- [x] `contracts/api-specification.yaml` OpenAPI 3.0 spec with 30+ endpoints

**Database Schema Highlights**:
- **VoiceCommand**: Stores audio + Whisper transcription + confidence score
- **VisualScene**: GPT-4 Vision analysis + detected objects (cached for 30s)
- **LLMQuery**: Natural language prompts + GPT-4 responses + token usage
- **RobotAction**: Validated actions with parameters (JSONB), execution status
- **TaskPlan**: Multi-step plans with replanning support
- **CapstoneProject**: 6-phase completion tracking, demo-ready flag

**API Structure**: REST + WebSocket
- REST: Request-response (voice transcription, vision analysis, code generation)
- WebSocket: Real-time robot state streaming (position, sensor data, action progress)

---

### Phase 2: Quickstart & Developer Guide ‚úÖ COMPLETE

**Deliverables**:
- [x] `quickstart.md` with 13-day development timeline
- [x] Architecture diagram (browser ‚Üí FastAPI ‚Üí ROS 2 ‚Üí Gazebo)
- [x] Code snippets for key implementations (Whisper, GPT-4 Vision, safety validation, code sandbox)
- [x] Testing strategy (unit, integration, end-to-end)
- [x] Common issues & solutions
- [x] Quality gates checklist

**13-Day Timeline Summary**:
- **Week 1** (Days 1-7): Core AI integration (voice, vision, LLM command processing, integration testing)
- **Week 2** (Days 8-13): Features & capstone (code generation, capstone project, playground, polish, documentation, deployment)

---

### Phase 3: Implementation Tasks (NEXT STEP)

**To Be Generated**: Run `/sp.tasks` command to create `tasks.md` with:
- Granular implementation tasks (2-4 hour chunks)
- Dependencies between tasks
- Acceptance criteria for each task
- Test cases (TDD approach)
- Priority labels (P0 blocker ‚Üí P3 nice-to-have)

**Expected Task Count**: ~60-80 tasks over 13 days (~5-6 tasks per day, accounting for testing & bug fixes)

---

## Critical Path (Must-Have for Demo)

**P0 (Blockers)**: These MUST work for demo
1. Voice command input (browser audio capture + Whisper transcription)
2. Natural language to robot action (GPT-4 command parsing + safety validation)
3. Robot execution in Gazebo (ROS 2 Twist publishing, visible movement)
4. Vision scene analysis (GPT-4 Vision, object identification)
5. Capstone project (all 6 phases functional, 2-3 hour completion verified)
6. 90-second demo video (pre-recorded fallback)

**P1 (Important)**: Enhance demo quality
- Code generation (Monaco Editor + GPT-4 code gen)
- Playground (prompt engineering experimentation)
- Multi-modal RAG chatbot (Chapter 3 Q&A)
- Framer Motion animations (smooth UI transitions)

**P2 (Nice-to-Have)**: Can be cut if time constrained
- Urdu translation for Chapter 3 content
- Advanced error recovery (LLM replanning on task failures)
- Code debugging assistance (AI-powered error diagnosis)
- Demo dashboard (real-time metrics during presentation)

---

## Risk Register

| Risk | Impact | Probability | Mitigation Status |
|------|--------|-------------|-------------------|
| OpenAI API rate limits during demo | High | Medium (30%) | ‚úÖ Pre-warm cache + request rate limit increase |
| Gazebo crashes mid-demo | High | Medium (25%) | ‚úÖ Pre-recorded fallback video + Three.js backup |
| Voice recognition fails (background noise) | Medium | Low (15%) | ‚úÖ High-quality USB mic + text input fallback |
| LLM generates invalid robot commands | Medium | Low (10%) | ‚úÖ Safety validation layer + GPT-4 temperature=0 |
| Capstone too complex (>3 hours) | Medium | Medium (20%) | ‚ö†Ô∏è  Test with 3 learners, simplify if needed |
| 13-day timeline slips | High | Medium (35%) | ‚ö†Ô∏è  Parallel development streams + cut P2 features if needed |

---

## Phase 2 Completion Checklist

- [x] Project structure designed (frontend, backend, ROS 2 workspace, docker)
- [x] Development workflow documented (13-day plan with daily breakdown)
- [x] Quickstart guide written (setup, key code snippets, testing strategy)
- [x] Critical path identified (P0 must-haves for demo)
- [x] Risk mitigation strategies documented
- [x] Quality gates defined (pre-deployment checklist)

**Status**: Ready for `/sp.tasks` to generate implementation tasks ‚úÖ

---

## Next Steps

1. **Run `/sp.tasks`**: Generate granular implementation tasks from this plan
2. **Begin Day 1**: Voice Control Foundation (browser audio capture + Whisper API)
3. **Daily Standup**: Review progress against 13-day timeline, adjust if needed
4. **Weekly Demo**: Friday Week 1 (voice‚Üírobot pipeline), Friday Week 2 (full capstone)
5. **Final Rehearsal**: Day 12 evening - rehearse 90-second demo, time all transitions

**Remember**: This is the climax chapter. Every detail matters for the 90-second demo! üöÄ
